{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM8BnWeRjSNzRvIRwH6tliL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlugubellySaisri/diabetes/blob/main/Week5%20part2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "TVH8gXM0bBTo",
        "outputId": "46cde121-d45f-40de-f2d2-6cea01c515e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-genai in /usr/local/lib/python3.12/dist-packages (1.48.0)\n",
            "Collecting google-genai\n",
            "  Downloading google_genai-1.49.0-py3-none-any.whl.metadata (46 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/46.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.7/46.7 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting arxiv\n",
            "  Downloading arxiv-2.3.0-py3-none-any.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (4.13.5)\n",
            "Collecting beautifulsoup4\n",
            "  Downloading beautifulsoup4-4.14.2-py3-none-any.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Collecting requests\n",
            "  Downloading requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting readability-lxml\n",
            "  Downloading readability_lxml-0.8.4.1-py3-none-any.whl.metadata (4.0 kB)\n",
            "Requirement already satisfied: anyio<5.0.0,>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from google-genai) (4.11.0)\n",
            "Requirement already satisfied: google-auth<3.0.0,>=2.14.1 in /usr/local/lib/python3.12/dist-packages (from google-genai) (2.38.0)\n",
            "Requirement already satisfied: httpx<1.0.0,>=0.28.1 in /usr/local/lib/python3.12/dist-packages (from google-genai) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.9.0 in /usr/local/lib/python3.12/dist-packages (from google-genai) (2.11.10)\n",
            "Requirement already satisfied: tenacity<9.2.0,>=8.2.3 in /usr/local/lib/python3.12/dist-packages (from google-genai) (8.5.0)\n",
            "Requirement already satisfied: websockets<15.1.0,>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from google-genai) (15.0.1)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.11.0 in /usr/local/lib/python3.12/dist-packages (from google-genai) (4.15.0)\n",
            "Collecting feedparser~=6.0.10 (from arxiv)\n",
            "  Downloading feedparser-6.0.12-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (2.8)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.10.5)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.12/dist-packages (from readability-lxml) (5.2.0)\n",
            "Requirement already satisfied: lxml[html_clean] in /usr/local/lib/python3.12/dist-packages (from readability-lxml) (5.4.0)\n",
            "Collecting cssselect (from readability-lxml)\n",
            "  Downloading cssselect-1.3.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0.0,>=4.8.0->google-genai) (1.3.1)\n",
            "Collecting sgmllib3k (from feedparser~=6.0.10->arxiv)\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.14.1->google-genai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.14.1->google-genai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.14.1->google-genai) (4.9.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0,>=0.28.1->google-genai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0,>=0.28.1->google-genai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.9.0->google-genai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.9.0->google-genai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.9.0->google-genai) (0.4.2)\n",
            "Collecting lxml_html_clean (from lxml[html_clean]->readability-lxml)\n",
            "  Downloading lxml_html_clean-0.4.3-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0,>=2.14.1->google-genai) (0.6.1)\n",
            "Downloading google_genai-1.49.0-py3-none-any.whl (256 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m256.1/256.1 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading arxiv-2.3.0-py3-none-any.whl (11 kB)\n",
            "Downloading beautifulsoup4-4.14.2-py3-none-any.whl (106 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.4/106.4 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests-2.32.5-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading readability_lxml-0.8.4.1-py3-none-any.whl (19 kB)\n",
            "Downloading feedparser-6.0.12-py3-none-any.whl (81 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.5/81.5 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cssselect-1.3.0-py3-none-any.whl (18 kB)\n",
            "Downloading lxml_html_clean-0.4.3-py3-none-any.whl (14 kB)\n",
            "Building wheels for collected packages: sgmllib3k\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6046 sha256=f998111211bb504d7ab5039d6fb9e69ebdc7ec21a76588f160c1dab28cb8b52f\n",
            "  Stored in directory: /root/.cache/pip/wheels/03/f5/1a/23761066dac1d0e8e683e5fdb27e12de53209d05a4a37e6246\n",
            "Successfully built sgmllib3k\n",
            "Installing collected packages: sgmllib3k, requests, lxml_html_clean, feedparser, cssselect, beautifulsoup4, arxiv, readability-lxml, google-genai\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.32.4\n",
            "    Uninstalling requests-2.32.4:\n",
            "      Successfully uninstalled requests-2.32.4\n",
            "  Attempting uninstall: beautifulsoup4\n",
            "    Found existing installation: beautifulsoup4 4.13.5\n",
            "    Uninstalling beautifulsoup4-4.13.5:\n",
            "      Successfully uninstalled beautifulsoup4-4.13.5\n",
            "  Attempting uninstall: google-genai\n",
            "    Found existing installation: google-genai 1.48.0\n",
            "    Uninstalling google-genai-1.48.0:\n",
            "      Successfully uninstalled google-genai-1.48.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed arxiv-2.3.0 beautifulsoup4-4.14.2 cssselect-1.3.0 feedparser-6.0.12 google-genai-1.49.0 lxml_html_clean-0.4.3 readability-lxml-0.8.4.1 requests-2.32.5 sgmllib3k-1.0.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "a30cb8860f3a4d0ca90c479a6c2ffb4a"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Google Gen AI SDK + utilities\n",
        "!pip install --upgrade google-genai arxiv beautifulsoup4 requests readability-lxml"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from getpass import getpass\n",
        "import os"
      ],
      "metadata": {
        "id": "0raCxBFLbyqS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "api_key = getpass(\"Paste your Google Gemini API key here: \")\n",
        "os.environ[\"GEMINI_API_KEY\"] = api_key   # optional - keep in memory only"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mXXcbyFLb1Kh",
        "outputId": "64ffd6e9-b074-4411-ff78-ffcc53eef56b"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Paste your Google Gemini API key here: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google import genai"
      ],
      "metadata": {
        "id": "mKbfFJ94b9xB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "|from google import genai\n",
        "\n",
        "# create client using the key we provided\n",
        "client = genai.Client(api_key=os.environ[\"GEMINI_API_KEY\"])\n"
      ],
      "metadata": {
        "id": "UaNUw2EldjMF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resp = client.models.generate_content(\n",
        "    model=\"gemini-2.5-flash\",\n",
        "    contents=\"Summarize in one sentence: Why is reproducibility important in research?\",\n",
        ")\n",
        "print(resp.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R5pqZ6XWdlV0",
        "outputId": "429f062e-0fce-48f5-d98a-ed2a8c9dba68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reproducibility is important because it validates research findings, ensuring their reliability and building the trust necessary for scientific progress.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import arxiv\n",
        "\n",
        "def search_arxiv(query, max_results=5):\n",
        "    search = arxiv.Search(\n",
        "        query=query,\n",
        "        max_results=max_results,\n",
        "        sort_by=arxiv.SortCriterion.Relevance\n",
        "    )\n",
        "    results = []\n",
        "    for result in search.results():\n",
        "        results.append({\n",
        "            \"title\": result.title,\n",
        "            \"summary\": result.summary,\n",
        "            \"authors\": [a.name for a in result.authors],\n",
        "            \"pdf_url\": result.pdf_url,\n",
        "            \"id\": result.get_short_id()\n",
        "        })\n",
        "    return results"
      ],
      "metadata": {
        "id": "TzIdHZwTdo-z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# quick test\n",
        "papers = search_arxiv(\"multimodal transformers\", max_results=3)\n",
        "for p in papers:\n",
        "    print(p['title'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wFt4zC9idvJU",
        "outputId": "f280ace1-500d-4a61-dd88-bcc7d57ba29a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-452329443.py:10: DeprecationWarning: The 'Search.results' method is deprecated, use 'Client.results' instead\n",
            "  for result in search.results():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Multimodal Learning with Transformers: A Survey\n",
            "MANGO: Multimodal Attention-based Normalizing Flow Approach to Fusion Learning\n",
            "Multimodal Transformer With a Low-Computational-Cost Guarantee\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def fetch_page_text(url, max_chars=4000):\n",
        "    try:\n",
        "        r = requests.get(url, timeout=10, headers={\"User-Agent\": \"research-agent/1.0\"})\n",
        "        r.raise_for_status()\n",
        "    except Exception as e:\n",
        "        return \"\"\n",
        "\n",
        "    soup = BeautifulSoup(r.text, \"html.parser\")\n",
        "    # simple extraction: join visible <p> text\n",
        "    paragraphs = [p.get_text(separator=\" \", strip=True) for p in soup.find_all(\"p\")]\n",
        "    content = \"\\n\".join(paragraphs)\n",
        "    return content[:max_chars] # limit length for API"
      ],
      "metadata": {
        "id": "yLRII-hRd-cV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chunk_text(text, max_chars=3000):\n",
        "    chunks = []\n",
        "    start = 0\n",
        "    while start < len(text):\n",
        "        end = min(len(text), start + max_chars)\n",
        "        chunks.append(text[start:end])\n",
        "        start = end\n",
        "    return chunks"
      ],
      "metadata": {
        "id": "DvtXnvuJeOqm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SYSTEM_PROMPT = (\n",
        "    \"You are a concise research assistant. For each text provided, return: \"\n",
        "    \"1) a one-paragraph summary (3-5 sentences), 2) three bullet key contributions/findings, \"\n",
        "    \"and 3) a suggested short title. Be factual and include no hallucinated facts.\"\n",
        ")\n",
        "\n",
        "def summarize_chunk(chunk_text):\n",
        "    prompt = SYSTEM_PROMPT + \"\\nNext to summarize:\\n\" + chunk_text\n",
        "    resp = client.models.generate_content(\n",
        "        model=\"gemini-2.5-flash\",  # adjust if unavailable\n",
        "        contents=prompt\n",
        "    )\n",
        "    return resp.text"
      ],
      "metadata": {
        "id": "noY8P7SKeZs4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize_text_long(text):\n",
        "    chunks = chunk_text(text, max_chars=3000)\n",
        "    summaries = []\n",
        "    for c in chunks:\n",
        "        s = summarize_chunk(c)\n",
        "        summaries.append(s)\n",
        "\n",
        "    # Optionally aggregate the chunk summaries into a single final summary:\n",
        "    if len(summaries) == 1:\n",
        "        return summaries[0]\n",
        "    else:\n",
        "        combined = \"\\n\\n\".join(summaries)\n",
        "        final_prompt = SYSTEM_PROMPT + \"\\n\\nCombine the following chunk summaries into a single concise summary:\\n\\n\" + combined\n",
        "        final_resp = client.models.generate_content(model=\"gemini-2.5-flash\", contents=final_prompt)\n",
        "        return final_resp.text"
      ],
      "metadata": {
        "id": "ZVpEbemMecTA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"multimodal transformers\"\n",
        "papers = search_arxiv(query, max_results=5)\n",
        "\n",
        "for p in papers:\n",
        "    print(\"\\n---\")\n",
        "    print(\"Title:\", p['title'])\n",
        "    text_to_summarize = p['summary'] # arXiv abstract (short)\n",
        "    summary = summarize_text_long(text_to_summarize)\n",
        "    print(\"Summary:\\n\", summary)\n",
        "    print(\"PDF:\", p['pdf_url'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o-6RGQNeemrh",
        "outputId": "0f1a30a9-63cb-4a57-9487-a0adc3b9e2bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-452329443.py:10: DeprecationWarning: The 'Search.results' method is deprecated, use 'Client.results' instead\n",
            "  for result in search.results():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "---\n",
            "Title: Multimodal Learning with Transformers: A Survey\n",
            "Summary:\n",
            " 1) This paper presents a comprehensive survey of Transformer techniques applied to multimodal data, an emerging field in AI research driven by multimodal applications and big data. The survey begins with a background on multimodal learning, the Transformer ecosystem, and the multimodal big data era. It then provides a theoretical review of Vanilla, Vision, and multimodal Transformers from a geometrically topological perspective, followed by an examination of their applications in multimodal pretraining and specific tasks. The paper concludes by summarizing common challenges and designs, and discussing open problems and future research directions for the community.\n",
            "\n",
            "2)\n",
            "*   Provides a comprehensive survey specifically focused on Transformer techniques for multimodal data.\n",
            "*   Includes a theoretical review of various Transformer architectures (Vanilla, Vision, and Multimodal) from a geometrically topological perspective.\n",
            "*   Covers multimodal Transformer applications in pretraining and specific tasks, alongside common challenges, designs, and future research directions.\n",
            "\n",
            "3) A Comprehensive Survey of Multimodal Transformers\n",
            "PDF: http://arxiv.org/pdf/2206.06488v2\n",
            "\n",
            "---\n",
            "Title: MANGO: Multimodal Attention-based Normalizing Flow Approach to Fusion Learning\n",
            "Summary:\n",
            " 1) This paper addresses the limitations of current multimodal fusion methods, which often use implicit Transformer attention and fail to capture essential features or complex correlations within multimodal data. It introduces MANGO (Multimodal Attention-based Normalizing Flow), a novel approach designed for explicit, interpretable, and tractable multimodal fusion learning. MANGO utilizes a new Invertible Cross-Attention (ICA) layer within a Normalizing Flow-based Model. To enhance correlation capture, the ICA layer incorporates three new cross-attention mechanisms: Modality-to-Modality (MMCA), Inter-Modality (IMCA), and Learnable Inter-Modality (LICA) Cross-Attention. The proposed method also scales to high-dimensional data and achieves state-of-the-art performance across diverse tasks like semantic segmentation, image-to-image translation, and movie genre classification.\n",
            "\n",
            "2) Key Contributions/Findings:\n",
            "*   Introduction of MANGO (Multimodal Attention-based Normalizing Flow) for explicit, interpretable, and tractable multimodal fusion.\n",
            "*   Development of a novel Invertible Cross-Attention (ICA) layer integrated with Normalizing Flows for multimodal data processing.\n",
            "*   Proposal of three new cross-attention mechanisms—MMCA, IMCA, and LICA—to efficiently capture complex correlations in multimodal data.\n",
            "\n",
            "3) Suggested Short Title: MANGO: Explicit Multimodal Fusion with Invertible Cross-Attention\n",
            "PDF: http://arxiv.org/pdf/2508.10133v1\n",
            "\n",
            "---\n",
            "Title: Multimodal Transformer With a Low-Computational-Cost Guarantee\n",
            "Summary:\n",
            " 1) **Summary:**\n",
            "Transformer models excel in multimodal tasks but face significant computational challenges due to the quadratic complexity of multi-head attention, especially with increasing modalities. To mitigate this, the Low-Cost Multimodal Transformer (LoCoMT) is introduced, proposing a novel attention mechanism that reduces computational cost during both training and inference. LoCoMT achieves this by assigning distinct multimodal attention patterns to each attention head, allowing for flexible control over multimodal signals while theoretically ensuring reduced cost. Experimental evaluations on Audioset and MedVidCL datasets demonstrate that LoCoMT not only reduces GFLOPs but also matches or surpasses the performance of established models.\n",
            "\n",
            "2) **Key Contributions/Findings:**\n",
            "*   LoCoMT addresses the quadratic complexity issue of multi-head attention in multimodal Transformers by introducing a novel, cost-efficient attention mechanism.\n",
            "*   It utilizes distinct multimodal attention patterns for each head, enabling flexible control of multimodal signals and theoretical computational cost reduction.\n",
            "*   Experimental results show LoCoMT reduces GFLOPs and achieves competitive or superior performance on multimodal datasets like Audioset and MedVidCL.\n",
            "\n",
            "3) **Suggested Short Title:**\n",
            "LoCoMT: Low-Cost Multimodal Transformer\n",
            "PDF: http://arxiv.org/pdf/2402.15096v1\n",
            "\n",
            "---\n",
            "Title: Brain encoding models based on multimodal transformers can transfer across language and vision\n",
            "Summary:\n",
            " 1) This study investigates how multimodal transformers can bridge the representation of concepts in language and vision within the human brain, addressing the limitation of traditional encoding models that train on modalities in isolation. Researchers trained encoding models using representations from multimodal transformers, demonstrating their ability to transfer across fMRI responses to stories and movies. The findings show that models trained on one modality successfully predict brain responses to the other, especially in cortical regions for conceptual meaning, revealing shared semantic dimensions. This work highlights how multimodal transformers offer superior alignment of language and vision concept representations compared to unimodal transformers, providing insights into the brain's multimodal processing.\n",
            "\n",
            "2) Key Contributions/Findings:\n",
            "*   Encoding models trained with multimodal transformer representations can successfully transfer predictions across fMRI responses to language (stories) and vision (movies).\n",
            "*   This cross-modal transfer is most effective in cortical regions representing conceptual meaning, indicating shared semantic dimensions underlying language and vision representations.\n",
            "*   Multimodal transformers learn more aligned representations of concepts in language and vision compared to unimodal transformers.\n",
            "\n",
            "3) Suggested Short Title: Multimodal Transformers Bridge Language and Vision Brain Representations\n",
            "PDF: http://arxiv.org/pdf/2305.12248v1\n",
            "\n",
            "---\n",
            "Title: Rethinking Vision Transformer and Masked Autoencoder in Multimodal Face Anti-Spoofing\n",
            "Summary:\n",
            " Here's the summary of the provided text:\n",
            "\n",
            "1)  This paper investigates fundamental aspects of Vision Transformers (ViT) for multimodal face anti-spoofing (FAS) using RGB, Infrared, and Depth data, focusing on inputs, pre-training, and finetuning strategies. The authors discover that local feature descriptors enhance ViT performance specifically for the IR modality. To improve finetuning efficiency, they introduce an Adaptive Multimodal Adapter (AMA) that aggregates local features while largely freezing ViT parameters. Furthermore, they propose M$^{2}$A$^{2}$E, a modality-asymmetric masked autoencoder, for self-supervised pre-training to learn superior task-aware and modality-agnostic representations, addressing the sub-optimality of generic pre-trained models. Extensive experiments confirm the effectiveness of these proposed methods across various unimodal and multimodal FAS settings.\n",
            "\n",
            "2)  **Key Contributions/Findings:**\n",
            "    *   Identified that local feature descriptors are beneficial for ViT performance on the Infrared (IR) modality but not RGB or Depth in multimodal FAS.\n",
            "    *   Designed an Adaptive Multimodal Adapter (AMA) for efficient finetuning, allowing effective aggregation of local multimodal features while preserving most ViT parameters.\n",
            "    *   Proposed M$^{2}$A$^{2}$E, a modality-asymmetric masked autoencoder, for self-supervised pre-training, which learns intrinsic task-aware and modality-agnostic representations superior to generic pre-training for multimodal FAS.\n",
            "\n",
            "3)  **Suggested Short Title:**\n",
            "    ViT Fundamentals for Multimodal Face Anti-Spoofing: Inputs, AMA, and M²A²E\n",
            "PDF: http://arxiv.org/pdf/2302.05744v1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "out_path = '/content/drive/MyDrive/research_summaries.txt'\n",
        "with open(out_path, 'w', encoding='utf-8') as f:\n",
        "    for p in papers:\n",
        "        f.write(\"TITLE: \" + p['title'] + \"\\n\")\n",
        "        s = summarize_text_long(p['summary'])\n",
        "        f.write(s + \"\\n\\n---\\n\\n\")\n",
        "\n",
        "print(\"Saved to\", out_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GNyx0uJsexyB",
        "outputId": "95898913-47a2-4e02-a1d2-23bb98ce3908"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Saved to /content/drive/MyDrive/research_summaries.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Z1S9emkifntd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}